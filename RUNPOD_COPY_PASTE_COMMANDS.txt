# RUNPOD QUICK SETUP - Copy and Paste Each Section
# ================================================

# ============================================
# SECTION 1: Install Basic Packages
# ============================================
apt-get update -qq && \
apt-get install -y python3-pip git wget curl -qq && \
pip3 install --upgrade pip

# ============================================
# SECTION 2: Install Hugging Face CLI
# ============================================
pip3 install huggingface-hub

# ============================================
# SECTION 3: Login to Hugging Face
# ============================================
# Get your token from: https://huggingface.co/settings/tokens
# Request Llama access: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct

huggingface-cli login
# Paste your token when prompted

# Verify login
huggingface-cli whoami

# ============================================
# SECTION 4: Install PyTorch and ML Libraries
# ============================================
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 && \
pip3 install transformers accelerate bitsandbytes scipy sentencepiece && \
pip3 install fastapi uvicorn python-multipart pydantic

# ============================================
# SECTION 5: Download Llama 3.1 8B Model
# ============================================
cd /workspace/models

python3 << 'EOF'
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

print("ðŸ”§ Setting up 4-bit quantization...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

print("ðŸ“¥ Downloading model (this takes 5-10 minutes)...")
model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    cache_dir="/workspace/models"
)

tokenizer = AutoTokenizer.from_pretrained(
    model_id, 
    cache_dir="/workspace/models"
)

print(f"âœ… Downloaded! Size: {model.get_memory_footprint() / 1e9:.2f} GB")

# Quick test
inputs = tokenizer("Hello", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=5)
print(f"âœ… Test: {tokenizer.decode(outputs[0])}")
EOF

# ============================================
# SECTION 6: Create Server Script
# ============================================
cd /workspace/llm_server

cat > llm_server.py << 'EOF'
#!/usr/bin/env python3
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Llama 3.1 8B Server")

model = None
tokenizer = None

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: Optional[int] = 512
    temperature: Optional[float] = 0.7

@app.on_event("startup")
async def load_model():
    global model, tokenizer
    logger.info("ðŸš€ Loading Llama 3.1 8B...")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",
        cache_dir="/workspace/models"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir="/workspace/models")
    tokenizer.pad_token = tokenizer.eos_token
    
    logger.info(f"âœ… Model loaded! Memory: {model.get_memory_footprint() / 1e9:.2f} GB")

@app.get("/")
async def root():
    return {"status": "online", "model": "Llama 3.1 8B 4-bit"}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if model else "loading",
        "model_loaded": model is not None,
        "gpu": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "none"
    }

@app.post("/generate")
async def generate(request: GenerateRequest):
    if not model:
        raise HTTPException(503, "Model not loaded")
    
    inputs = tokenizer(request.prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens=request.max_tokens,
        temperature=request.temperature,
        do_sample=True
    )
    
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if text.startswith(request.prompt):
        text = text[len(request.prompt):].strip()
    
    return {
        "generated_text": text,
        "model": "Llama-3.1-8B-4bit",
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
EOF

chmod +x llm_server.py

# ============================================
# SECTION 7: Start Server in Background
# ============================================
cd /workspace/llm_server

# Start server with nohup
nohup python3 llm_server.py > /workspace/logs/llm_server.log 2>&1 &

# Save process ID
echo $! > /workspace/llm_server.pid

echo "Server started! PID: $(cat /workspace/llm_server.pid)"

# Wait for startup
sleep 10

# Check logs
tail -20 /workspace/logs/llm_server.log

# ============================================
# SECTION 8: Test the Server
# ============================================
# Test health
curl http://localhost:8000/health

# Test generation
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "The best place in Istanbul is", "max_tokens": 50}'

# ============================================
# USEFUL COMMANDS
# ============================================

# View logs
tail -f /workspace/logs/llm_server.log

# Check if running
ps -p $(cat /workspace/llm_server.pid)

# Stop server
kill $(cat /workspace/llm_server.pid)

# Restart server
kill $(cat /workspace/llm_server.pid)
sleep 3
nohup python3 /workspace/llm_server/llm_server.py > /workspace/logs/llm_server.log 2>&1 &
echo $! > /workspace/llm_server.pid

# Check GPU
nvidia-smi

# Monitor GPU
watch -n 1 nvidia-smi
