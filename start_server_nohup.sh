#!/bin/bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ START LLM SERVER WITH NOHUP - Persistent Background Process
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# This ensures the server keeps running even after SSH disconnect

set -e  # Exit on error

echo "ğŸš€ Starting LLM Server with nohup (persistent mode)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# Change to workspace
cd /workspace

# Check if model exists
if [ ! -d "/workspace/models" ]; then
    echo "âŒ Model directory not found!"
    echo "Run: ./download_model.sh"
    exit 1
fi

# Check if llm_server.py exists
if [ ! -f "/workspace/llm_server.py" ]; then
    echo "âŒ llm_server.py not found!"
    echo "Upload files first"
    exit 1
fi

# Install dependencies
echo "ğŸ“¦ Installing Python dependencies..."
pip install -q fastapi uvicorn[standard] transformers torch accelerate bitsandbytes pydantic requests 2>&1 | grep -v "already satisfied" || true
echo "âœ… Dependencies installed!"
echo ""

# Create logs directory
mkdir -p /workspace/logs

# Kill any existing server
echo "ğŸ” Stopping any existing servers..."
if [ -f "/workspace/llm_server.pid" ]; then
    OLD_PID=$(cat /workspace/llm_server.pid)
    if ps -p $OLD_PID > /dev/null 2>&1; then
        echo "   Killing old server (PID: $OLD_PID)"
        kill $OLD_PID 2>/dev/null || true
        sleep 2
    fi
fi

# Also kill by process name as backup
pkill -f "llm_server.py" 2>/dev/null || true
sleep 2

# Start server with nohup (survives SSH disconnect)
echo "ğŸ”„ Starting server with nohup..."
echo "   Command: nohup python /workspace/llm_server.py > /workspace/logs/llm_server.log 2>&1 &"
echo ""

nohup python /workspace/llm_server.py > /workspace/logs/llm_server.log 2>&1 &

# Get PID
SERVER_PID=$!
echo $SERVER_PID > /workspace/llm_server.pid

# Disown to ensure persistence
disown 2>/dev/null || true

echo "âœ… Server started!"
echo "   PID: $SERVER_PID"
echo "   Log: /workspace/logs/llm_server.log"
echo ""

# Wait for server to initialize
echo "â³ Waiting for model to load (30 seconds)..."
for i in {1..6}; do
    echo -n "."
    sleep 5
done
echo ""
echo ""

# Check if process is still running
if ps -p $SERVER_PID > /dev/null 2>&1; then
    echo "âœ… Server process is running!"
else
    echo "âŒ Server process stopped unexpectedly!"
    echo ""
    echo "Last 30 lines of log:"
    tail -n 30 /workspace/logs/llm_server.log
    exit 1
fi

# Test health endpoint
echo "ğŸ§ª Testing health endpoint..."
HEALTH_RESPONSE=$(curl -s http://localhost:8000/health 2>/dev/null || echo "")

if [ -n "$HEALTH_RESPONSE" ]; then
    echo "âœ… Server is responding!"
    echo ""
    echo "$HEALTH_RESPONSE" | python3 -m json.tool 2>/dev/null || echo "$HEALTH_RESPONSE"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ‰ LLM SERVER IS READY!"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    echo "ğŸ“Š Server Info:"
    echo "   â€¢ PID: $SERVER_PID"
    echo "   â€¢ Port: 8000"
    echo "   â€¢ Logs: /workspace/logs/llm_server.log"
    echo ""
    echo "ğŸ” Check status:"
    echo "   ps aux | grep llm_server.py"
    echo ""
    echo "ğŸ“œ View logs:"
    echo "   tail -f /workspace/logs/llm_server.log"
    echo ""
    echo "ğŸ§ª Test completion:"
    echo '   curl -X POST http://localhost:8000/v1/completions \'
    echo '     -H "Content-Type: application/json" \'
    echo '     -d '"'"'{"prompt": "Istanbul is", "max_tokens": 30}'"'"' | python3 -m json.tool'
    echo ""
    echo "ğŸ›‘ Stop server:"
    echo "   kill $(cat /workspace/llm_server.pid)"
    echo ""
    echo "âœ… Server will keep running even after you disconnect from SSH!"
    echo ""
else
    echo "âš ï¸  Server started but not responding yet"
    echo "   Model may still be loading..."
    echo ""
    echo "Check logs:"
    echo "   tail -f /workspace/logs/llm_server.log"
    echo ""
    echo "Wait 1-2 minutes and test again:"
    echo "   curl http://localhost:8000/health | python3 -m json.tool"
    echo ""
fi
