#!/bin/bash

# ๐ COMPLETE RUNPOD SETUP - Run this in RunPod Terminal
# This will install and start vLLM from scratch

set -e

echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
echo "๐ RunPod vLLM Complete Installation"
echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
echo ""

# Step 1: Update system
echo "๐ฆ Step 1: Updating system packages..."
apt-get update -qq

# Step 2: Install vLLM
echo ""
echo "๐ฆ Step 2: Installing vLLM..."
pip install --upgrade pip
pip install vllm

# Step 3: Check GPU
echo ""
echo "๐ฎ Step 3: Checking GPU..."
nvidia-smi
echo ""

# Step 4: Kill any existing vLLM
echo "๐งน Step 4: Cleaning up old processes..."
pkill -9 -f vllm 2>/dev/null || true
sleep 2

# Step 5: Start vLLM with conservative settings
echo ""
echo "๐ Step 5: Starting vLLM server..."
echo "   Model: Meta-Llama-3.1-8B-Instruct"
echo "   Port: 8000"
echo "   Max context: 2048 tokens"
echo "   GPU memory: 50%"
echo ""

nohup python3 -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Meta-Llama-3.1-8B-Instruct \
  --port 8000 \
  --host 0.0.0.0 \
  --dtype auto \
  --max-model-len 2048 \
  --gpu-memory-utilization 0.5 \
  --trust-remote-code \
  > /root/vllm.log 2>&1 &

VLLM_PID=$!
echo $VLLM_PID > /root/vllm.pid

echo "โ vLLM started with PID: $VLLM_PID"
echo ""

# Step 6: Wait for model to load
echo "โณ Step 6: Waiting for model to load (this takes 60-90 seconds)..."
echo ""

for i in {1..30}; do
    sleep 3
    ELAPSED=$((i * 3))
    
    # Show progress every 15 seconds
    if [ $((i % 5)) -eq 0 ]; then
        echo "   ${ELAPSED}s elapsed..."
    fi
    
    # Test every 9 seconds
    if [ $((i % 3)) -eq 0 ]; then
        RESPONSE=$(curl -s http://localhost:8000/v1/models 2>&1)
        if echo "$RESPONSE" | grep -q "Meta-Llama"; then
            echo ""
            echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
            echo "โ SUCCESS! vLLM is ready!"
            echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
            echo ""
            echo "๐ Model info:"
            echo "$RESPONSE" | python3 -m json.tool 2>/dev/null || echo "$RESPONSE"
            echo ""
            echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
            echo "๐ INSTALLATION COMPLETE!"
            echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
            echo ""
            echo "๐ Important Info:"
            echo "   โข PID: $VLLM_PID (saved in /root/vllm.pid)"
            echo "   โข Port: 8000"
            echo "   โข Logs: /root/vllm.log"
            echo ""
            echo "๐ง Useful Commands:"
            echo "   โข Test: curl http://localhost:8000/v1/models"
            echo "   โข Logs: tail -f /root/vllm.log"
            echo "   โข Stop: kill \$(cat /root/vllm.pid)"
            echo "   โข Status: ps aux | grep vllm"
            echo ""
            echo "โ You can now close this terminal - vLLM will keep running!"
            echo ""
            exit 0
        fi
    fi
done

# If we get here, it's taking longer than expected
echo ""
echo "โ๏ธ  vLLM is still loading (taking longer than usual)..."
echo ""
echo "Check status:"
echo "  ps aux | grep vllm | grep -v grep"
echo ""
echo "Check logs:"
echo "  tail -50 /root/vllm.log"
echo ""
echo "Test manually:"
echo "  curl http://localhost:8000/v1/models"
echo ""
echo "The process is running, but may need more time to load the model."
echo "Check back in a minute or two!"
echo ""
