#!/usr/bin/env python3
"""
Simple Llama 3.1 8B downloader for RunPod (30GB)
Avoids metadata issues by using cache_dir instead of local_dir
"""

from huggingface_hub import HfFolder, hf_hub_download
from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import sys

print("üöÄ Llama 3.1 8B Download (Simple Method)")
print("=" * 50)

# Get token
token = HfFolder.get_token()
if not token:
    print("‚ùå No HF token found. Run: huggingface-cli login")
    sys.exit(1)

MODEL_ID = "meta-llama/Llama-3.1-8B-Instruct"
CACHE_DIR = "/workspace/models"

print(f"üì• Downloading {MODEL_ID}...")
print(f"üìÅ Cache directory: {CACHE_DIR}")
print("‚è≥ This takes 5-10 minutes...")
print("")

try:
    # Method 1: Use transformers to download (handles everything)
    print("Step 1: Downloading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_ID,
        cache_dir=CACHE_DIR,
        token=token
    )
    print("‚úÖ Tokenizer downloaded!")
    
    print("")
    print("Step 2: Downloading model (~16GB)...")
    print("This is the large download, please wait...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        cache_dir=CACHE_DIR,
        token=token,
        torch_dtype="auto",
        device_map=None,  # Don't load to GPU yet
        low_cpu_mem_usage=True
    )
    print("‚úÖ Model downloaded!")
    
    print("")
    print("=" * 50)
    print("‚úÖ Download Complete!")
    print("=" * 50)
    print(f"üìÅ Model cached in: {CACHE_DIR}")
    print("")
    print("Model location:")
    print(f"  {MODEL_ID}")
    print("")
    
except Exception as e:
    print(f"‚ùå Download failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
