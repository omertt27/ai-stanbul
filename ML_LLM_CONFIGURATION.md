# ğŸ¤– LLM-First Configuration Guide

**Decision:** Use LLM by default for better quality answers  
**Created:** November 3, 2025  
**Priority:** HIGH

---

## ğŸ¯ Strategy: LLM-First with Smart Fallback

### Why LLM-First?
- âœ… **Better Quality:** More natural, context-aware responses
- âœ… **User Satisfaction:** Detailed, helpful answers
- âœ… **Flexibility:** Can handle complex queries
- âš ï¸ **Trade-off:** Slower response time (but acceptable for quality)

### Performance Impact

| Method | CPU (Local) | T4 GPU (Production) | Quality |
|--------|-------------|---------------------|---------|
| Template | <1s | <1s | Basic |
| LLM | ~18s | ~2-4s | Excellent â­ |

**Decision:** Use LLM by default, optimize for T4 GPU deployment

---

## ğŸš€ Configuration Changes

### 1. Update Backend Integration

**File:** `backend/main.py` or `app.py`

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ML CONFIGURATION - LLM BY DEFAULT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ML Service Configuration
ML_SERVICE_URL = os.getenv("ML_SERVICE_URL", "http://localhost:8000")
ML_SERVICE_TIMEOUT = float(os.getenv("ML_SERVICE_TIMEOUT", "60.0"))  # Increased for LLM
ML_SERVICE_ENABLED = os.getenv("ML_SERVICE_ENABLED", "true").lower() == "true"
ML_USE_LLM_DEFAULT = os.getenv("ML_USE_LLM_DEFAULT", "true").lower() == "true"  # LLM by default

print("\nğŸ¤– ML Service Configuration:")
print(f"   URL: {ML_SERVICE_URL}")
print(f"   Enabled: {ML_SERVICE_ENABLED}")
print(f"   Timeout: {ML_SERVICE_TIMEOUT}s")
print(f"   LLM Default: {ML_USE_LLM_DEFAULT}")
```

### 2. Update Chat Endpoint

```python
@app.post("/api/v1/chat", response_model=ChatResponse, tags=["Chat"])
async def chat_endpoint(request: ChatRequest):
    """
    Chat endpoint with LLM-first approach
    """
    start_time = time.time()
    
    try:
        # Detect intent
        detected_intent = "general"  # TODO: Use your intent classifier
        
        # Determine if should use LLM
        use_llm = request.use_llm if hasattr(request, 'use_llm') and request.use_llm is not None else ML_USE_LLM_DEFAULT
        
        logger.info(f"ğŸ’¬ Query: '{request.message}' (intent: {detected_intent}, llm: {use_llm})")
        
        # Try ML service
        ml_response = await get_ml_answer(
            query=request.message,
            intent=detected_intent,
            user_location=request.user_location,
            use_llm=use_llm,  # LLM by default
            language=request.language
        )
        
        if ml_response and ml_response.get('success'):
            # ML service succeeded âœ…
            logger.info(f"âœ… ML response: {ml_response.get('generation_method')} ({time.time() - start_time:.2f}s)")
            
            return ChatResponse(
                response=ml_response['answer'],
                intent=ml_response.get('intent', detected_intent),
                confidence=ml_response.get('confidence', 0.85),
                method=f"ml_{ml_response.get('generation_method', 'llm')}",
                context=ml_response.get('context', []),
                suggestions=ml_response.get('suggestions', []),
                response_time=time.time() - start_time,
                ml_service_used=True
            )
        
        # Fallback to rule-based
        logger.info("âš ï¸ ML service unavailable - using fallback")
        
        fallback = await generate_fallback_response(
            request.message,
            detected_intent,
            request.user_location
        )
        
        return ChatResponse(
            response=fallback['answer'],
            intent=detected_intent,
            confidence=0.6,
            method="fallback",
            context=fallback.get('context', []),
            suggestions=generate_suggestions(detected_intent),
            response_time=time.time() - start_time,
            ml_service_used=False
        )
    
    except Exception as e:
        logger.error(f"âŒ Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

### 3. Update Request Model

```python
class ChatRequest(BaseModel):
    """Chat request model"""
    message: str = Field(..., min_length=1, max_length=1000)
    user_location: Optional[Dict[str, float]] = None
    use_llm: Optional[bool] = Field(None, description="Override: Use LLM (None=use default)")
    language: str = Field(default="en", description="Response language (en/tr)")
    user_id: Optional[str] = None
```

### 4. Update Environment Variables

**File:** `.env`

```bash
# ML Service Configuration
ML_SERVICE_ENABLED=true
ML_SERVICE_URL=http://localhost:8000
ML_SERVICE_TIMEOUT=60.0  # Increased for LLM

# LLM Configuration - USE LLM BY DEFAULT
ML_USE_LLM_DEFAULT=true

# For Production T4 GPU (faster LLM)
# ML_SERVICE_URL=http://YOUR_T4_INSTANCE_IP:8000
# ML_SERVICE_TIMEOUT=30.0  # Can be lower on T4
```

---

## âš¡ Performance Optimization for LLM

### 1. Local Development (CPU)

**Expected:** ~18s per response

**Optimizations:**
```python
# In ml_systems/local_llm_generator.py

# Reduce max tokens for faster generation
max_tokens = 200  # Instead of 300

# Lower temperature for more focused responses
temperature = 0.7  # Instead of 0.8

# Disable sampling for fastest generation (deterministic)
do_sample = False
```

### 2. Production (T4 GPU) - RECOMMENDED

**Expected:** ~2-4s per response

**Steps:**
1. Deploy ML service to T4 GPU instance
2. Update `ML_SERVICE_URL` to T4 IP
3. Reduce timeout: `ML_SERVICE_TIMEOUT=30.0`
4. Enjoy 5-10x faster LLM responses! ğŸš€

---

## ğŸšï¸ Smart LLM Usage Strategy

### Option 1: Always LLM (Current)
```python
ML_USE_LLM_DEFAULT=true
```

**Best for:**
- Quality over speed
- T4 GPU deployment
- Low traffic (<100 req/min)

### Option 2: Intent-Based LLM
```python
# Use LLM only for complex intents
COMPLEX_INTENTS = [
    'route_planning',
    'local_tips',
    'neighborhood_info',
    'events_query'
]

# In chat endpoint:
use_llm = detected_intent in COMPLEX_INTENTS
```

**Best for:**
- Balance speed and quality
- High traffic
- Cost optimization

### Option 3: User Choice
```python
# Let user decide
class ChatRequest(BaseModel):
    message: str
    detail_level: str = Field(default="detailed", enum=["quick", "detailed"])

# In endpoint:
use_llm = request.detail_level == "detailed"
```

**Best for:**
- Power users
- Mobile apps (battery consideration)
- Freemium models

---

## ğŸš€ Deployment Strategy with LLM

### Phase 1: Local Testing (This Week)
```bash
# Expected: ~18s per response
ML_SERVICE_URL=http://localhost:8000
ML_USE_LLM_DEFAULT=true
ML_SERVICE_TIMEOUT=60.0
```

**Test with:**
- Low traffic
- Development/staging
- Quality validation

### Phase 2: T4 GPU Production (Next Week)
```bash
# Expected: ~2-4s per response
ML_SERVICE_URL=http://YOUR_T4_IP:8000
ML_USE_LLM_DEFAULT=true
ML_SERVICE_TIMEOUT=30.0
```

**Benefits:**
- âœ… 5-10x faster LLM
- âœ… Handle high traffic
- âœ… Better user experience
- ğŸ’° Cost: ~$0.35/hour

---

## ğŸ“Š Expected Performance

### Local Development (CPU)
```
User Query â†’ Backend â†’ ML Service (CPU) â†’ LLM Generation
                                            (~18s)
                                              â†“
User â† Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Answer
(Total: ~18-19s)
```

### Production (T4 GPU)
```
User Query â†’ Backend â†’ ML Service (T4 GPU) â†’ LLM Generation
                                               (~2-4s)
                                                 â†“
User â† Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Answer
(Total: ~2.5-4.5s)
```

---

## ğŸ¯ Updated Integration Steps

### Quick Start with LLM

1. **Update `.env`:**
```bash
ML_SERVICE_ENABLED=true
ML_SERVICE_URL=http://localhost:8000
ML_SERVICE_TIMEOUT=60.0
ML_USE_LLM_DEFAULT=true  # LLM by default
```

2. **Update Backend:**
```python
# Copy code from PASTE_INTO_BACKEND.py
# Make sure use_llm defaults to True
```

3. **Start Services:**
```bash
# Terminal 1: ML Service
source venv_ml/bin/activate
./start_ml_service.sh

# Terminal 2: Backend
cd backend
python main.py
```

4. **Test:**
```bash
# Test with LLM (detailed)
curl -X POST http://localhost:YOUR_PORT/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What are the best seafood restaurants in BeyoÄŸlu with Bosphorus view?"
  }'

# Expected: Detailed, natural response (~18s on CPU)
```

---

## ğŸ’¡ Pro Tips for LLM Usage

### 1. Add Loading Indicator
```python
# Frontend should show:
# "ğŸ¤– Thinking... (this may take a moment for detailed answers)"
```

### 2. Stream Responses (Advanced)
```python
@app.post("/api/v1/chat/stream")
async def chat_stream(request: ChatRequest):
    """Stream LLM response for better UX"""
    # TODO: Implement streaming
    # Tokens appear as they're generated
```

### 3. Cache Common Queries
```python
# Cache frequently asked questions
CACHED_QUERIES = {
    "best restaurants": {...},
    "top attractions": {...},
    "how to get around": {...}
}
```

### 4. Monitor Response Times
```python
from prometheus_client import Histogram

llm_response_time = Histogram(
    'llm_response_seconds',
    'LLM response time',
    buckets=[1, 2, 5, 10, 20, 30, 60]
)

# In endpoint:
with llm_response_time.time():
    ml_response = await get_ml_answer(...)
```

---

## ğŸš¨ Important Considerations

### 1. Timeout Configuration
```python
# Set appropriate timeouts
ML_SERVICE_TIMEOUT = 60.0  # Local CPU
ML_SERVICE_TIMEOUT = 30.0  # T4 GPU
ML_SERVICE_TIMEOUT = 90.0  # Very complex queries
```

### 2. Fallback Strategy
```python
async def get_ml_answer_with_retry(query, intent, **kwargs):
    """Try LLM, fallback to template on timeout"""
    try:
        return await get_ml_answer(query, intent, use_llm=True, **kwargs)
    except TimeoutException:
        logger.warning("LLM timeout, trying template")
        return await get_ml_answer(query, intent, use_llm=False, **kwargs)
```

### 3. Cost Monitoring (T4 GPU)
```python
# Track GPU usage
import psutil
import GPUtil

def log_gpu_usage():
    gpus = GPUtil.getGPUs()
    for gpu in gpus:
        logger.info(f"GPU {gpu.id}: {gpu.load*100:.1f}% load, {gpu.temperature}Â°C")
```

---

## âœ… Updated Checklist

- [ ] Update `.env` with LLM configuration
- [ ] Set `ML_USE_LLM_DEFAULT=true`
- [ ] Increase timeout to 60s (CPU) or 30s (GPU)
- [ ] Update backend to use LLM by default
- [ ] Test locally (expect ~18s on CPU)
- [ ] Add loading indicators in frontend
- [ ] Plan T4 GPU deployment (for production speed)
- [ ] Monitor response times
- [ ] Test with real user queries
- [ ] Collect feedback on answer quality

---

## ğŸ‰ Summary

### Configuration
âœ… **LLM by default** for best quality  
âœ… **60s timeout** for local CPU  
âœ… **Optional override** per request  
âœ… **Fallback** if ML unavailable  

### Performance
âš ï¸ **Local:** ~18s per response (acceptable for quality)  
âœ… **T4 GPU:** ~2-4s per response (production-ready)  

### Next Steps
1. âœ… Configure LLM as default
2. âœ… Test locally with LLM
3. âœ… Deploy to T4 GPU for production speed
4. âœ… Monitor and optimize

---

**The quality of LLM responses is worth the wait! And with T4 GPU, you get both quality AND speed! ğŸš€**
