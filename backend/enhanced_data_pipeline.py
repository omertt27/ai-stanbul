#!/usr/bin/env python3
"""
Enhanced Data Pipeline & ETL System for AI Istanbul
===================================================

Implements:
1. Automated data collection from APIs, open data, web scraping
2. Incremental updates to reduce processing costs
3. Data validation, normalization, and quality assurance
4. Integration with vector embeddings and full-text search
"""

import asyncio
import aiohttp
import schedule
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import sqlite3
import logging
from pathlib import Path
import hashlib

# Import our systems
try:
    from vector_embedding_system import vector_embedding_system
    VECTOR_SYSTEM_AVAILABLE = True
except ImportError:
    VECTOR_SYSTEM_AVAILABLE = False

try:
    from database_enhancements import database_enhancement_service
    DATABASE_ENHANCEMENTS_AVAILABLE = True
except ImportError:
    DATABASE_ENHANCEMENTS_AVAILABLE = False

logger = logging.getLogger(__name__)

class DataSource(Enum):
    GOOGLE_PLACES_API = "google_places"
    ISTANBUL_OPEN_DATA = "istanbul_open_data"
    WEB_SCRAPING = "web_scraping"
    SOCIAL_MEDIA = "social_media"
    GOVERNMENT_API = "government_api"
    MANUAL_CURATION = "manual_curation"

class DataQuality(Enum):
    EXCELLENT = "excellent"  # >0.9 - Auto-approve
    GOOD = "good"           # 0.7-0.9 - Manual review
    FAIR = "fair"           # 0.5-0.7 - Needs improvement
    POOR = "poor"           # <0.5 - Auto-reject

@dataclass
class DataRecord:
    """Standardized data record for processing"""
    id: str
    source: DataSource
    data_type: str  # restaurant, museum, event, transport
    raw_data: Dict[str, Any]
    normalized_data: Dict[str, Any]
    quality_score: float
    validation_errors: List[str]
    last_updated: datetime
    hash_signature: str

class EnhancedDataPipeline:
    """Complete data pipeline with incremental updates and quality assurance"""
    
    def __init__(self, db_path: str = "enhanced_pipeline.db"):
        self.db_path = db_path
        self.session = None
        self._init_database()
        
        # Data validation rules
        self.validation_rules = {
            "restaurant": {
                "required_fields": ["name", "location"],
                "optional_fields": ["cuisine_type", "rating", "price_level", "phone"],
                "validation_functions": [
                    self._validate_name,
                    self._validate_location,
                    self._validate_rating
                ]
            },
            "museum": {
                "required_fields": ["name", "location"],
                "optional_fields": ["category", "opening_hours", "rating"],
                "validation_functions": [
                    self._validate_name,
                    self._validate_location,
                    self._validate_opening_hours
                ]
            },
            "event": {
                "required_fields": ["name", "date", "location"],
                "optional_fields": ["description", "category", "price"],
                "validation_functions": [
                    self._validate_name,
                    self._validate_date,
                    self._validate_location
                ]
            }
        }
        
        # Istanbul districts for location validation
        self.istanbul_districts = [
            "Sultanahmet", "Beyoƒülu", "Galata", "Kadik√∂y", "Be≈üikta≈ü", "≈ûi≈üli",
            "Fatih", "Emin√∂n√º", "√úsk√ºdar", "Ortak√∂y", "Taksim", "Balat",
            "Fener", "Ni≈üanta≈üƒ±", "Bebek", "Arnavutk√∂y", "Sarƒ±yer", "Bakƒ±rk√∂y",
            "Zeytinburnu", "Ey√ºp", "Kaƒüƒ±thane", "Pendik", "Maltepe"
        ]
    
    def _init_database(self):
        """Initialize pipeline database"""
        with sqlite3.connect(self.db_path) as conn:
            # Data records table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS data_records (
                    id TEXT PRIMARY KEY,
                    source TEXT NOT NULL,
                    data_type TEXT NOT NULL,
                    raw_data TEXT NOT NULL,
                    normalized_data TEXT NOT NULL,
                    quality_score REAL NOT NULL,
                    validation_errors TEXT,
                    hash_signature TEXT NOT NULL,
                    status TEXT DEFAULT 'pending',
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Processing logs
            conn.execute("""
                CREATE TABLE IF NOT EXISTS processing_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    pipeline_run_id TEXT NOT NULL,
                    source TEXT NOT NULL,
                    records_processed INTEGER DEFAULT 0,
                    records_added INTEGER DEFAULT 0,
                    records_updated INTEGER DEFAULT 0,
                    records_rejected INTEGER DEFAULT 0,
                    errors TEXT,
                    duration_seconds REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Data source configurations
            conn.execute("""
                CREATE TABLE IF NOT EXISTS data_sources (
                    source TEXT PRIMARY KEY,
                    config TEXT NOT NULL,
                    last_update DATETIME,
                    update_frequency_hours INTEGER DEFAULT 24,
                    enabled BOOLEAN DEFAULT TRUE
                )
            """)
            
            # Create indexes
            conn.execute("CREATE INDEX IF NOT EXISTS idx_data_type ON data_records(data_type)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_source ON data_records(source)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_hash ON data_records(hash_signature)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_updated ON data_records(updated_at)")
            
            conn.commit()
    
    async def _get_session(self):
        """Get or create aiohttp session"""
        if not self.session:
            self.session = aiohttp.ClientSession()
        return self.session
    
    async def collect_google_places_data(self, query: str, data_type: str) -> List[DataRecord]:
        """Collect data from Google Places API"""
        print(f"üåê Collecting Google Places data for: {query}")
        
        # Simulated Google Places API response (replace with real API)
        simulated_data = []
        
        if data_type == "restaurant":
            simulated_data = [
                {
                    "place_id": "google_1",
                    "name": "Pandeli Restaurant",
                    "formatted_address": "Emin√∂n√º, Istanbul",
                    "rating": 4.3,
                    "price_level": 3,
                    "types": ["restaurant", "turkish_cuisine"],
                    "geometry": {"location": {"lat": 41.0157, "lng": 28.9737}}
                },
                {
                    "place_id": "google_2", 
                    "name": "Hamdi Et Lokantasƒ±",
                    "formatted_address": "Emin√∂n√º, Istanbul",
                    "rating": 4.5,
                    "price_level": 2,
                    "types": ["restaurant", "kebab"],
                    "geometry": {"location": {"lat": 41.0156, "lng": 28.9740}}
                }
            ]
        elif data_type == "museum":
            simulated_data = [
                {
                    "place_id": "google_3",
                    "name": "Istanbul Archaeological Museums",
                    "formatted_address": "Sultanahmet, Istanbul", 
                    "rating": 4.4,
                    "types": ["museum", "tourist_attraction"],
                    "geometry": {"location": {"lat": 41.0115, "lng": 28.9809}}
                }
            ]
        
        records = []
        for item in simulated_data:
            # Create standardized record
            record = DataRecord(
                id=f"google_{item['place_id']}",
                source=DataSource.GOOGLE_PLACES_API,
                data_type=data_type,
                raw_data=item,
                normalized_data=self._normalize_google_places_data(item, data_type),
                quality_score=0.0,  # Will be calculated
                validation_errors=[],
                last_updated=datetime.now(),
                hash_signature=self._calculate_hash(item)
            )
            
            # Validate and score
            record = self._validate_and_score_record(record)
            records.append(record)
        
        print(f"‚úÖ Collected {len(records)} Google Places records")
        return records
    
    async def collect_istanbul_open_data(self, data_type: str) -> List[DataRecord]:
        """Collect data from Istanbul's open data sources"""
        print(f"üèõÔ∏è Collecting Istanbul open data for: {data_type}")
        
        # Simulated open data (replace with real API calls)
        simulated_data = []
        
        if data_type == "transport":
            simulated_data = [
                {
                    "stop_id": "open_1",
                    "stop_name": "Sultanahmet Station", 
                    "stop_type": "metro",
                    "coordinates": [41.0061, 28.9777],
                    "lines": ["M1A"]
                },
                {
                    "stop_id": "open_2",
                    "stop_name": "Emin√∂n√º Ferry Terminal",
                    "stop_type": "ferry",
                    "coordinates": [41.0168, 28.9735],
                    "lines": ["Bosphorus", "Golden Horn"]
                }
            ]
        elif data_type == "event":
            simulated_data = [
                {
                    "event_id": "open_3",
                    "title": "Istanbul Art Biennial",
                    "start_date": "2025-10-15",
                    "end_date": "2025-12-15", 
                    "venue": "Various locations",
                    "category": "art_exhibition"
                }
            ]
        
        records = []
        for item in simulated_data:
            record = DataRecord(
                id=f"open_{item.get('stop_id', item.get('event_id', 'unknown'))}",
                source=DataSource.ISTANBUL_OPEN_DATA,
                data_type=data_type,
                raw_data=item,
                normalized_data=self._normalize_open_data(item, data_type),
                quality_score=0.0,
                validation_errors=[],
                last_updated=datetime.now(),
                hash_signature=self._calculate_hash(item)
            )
            
            record = self._validate_and_score_record(record)
            records.append(record)
        
        print(f"‚úÖ Collected {len(records)} open data records")
        return records
    
    def _normalize_google_places_data(self, data: Dict, data_type: str) -> Dict[str, Any]:
        """Normalize Google Places data to standard format"""
        normalized = {
            "name": data.get("name", ""),
            "location": data.get("formatted_address", ""),
            "rating": data.get("rating"),
            "coordinates": {
                "lat": data.get("geometry", {}).get("location", {}).get("lat"),
                "lng": data.get("geometry", {}).get("location", {}).get("lng")
            }
        }
        
        if data_type == "restaurant":
            normalized.update({
                "cuisine_type": self._extract_cuisine_type(data.get("types", [])),
                "price_level": data.get("price_level"),
                "phone": data.get("formatted_phone_number")
            })
        elif data_type == "museum":
            normalized.update({
                "category": "museum",
                "opening_hours": data.get("opening_hours", {}).get("weekday_text", [])
            })
        
        return normalized
    
    def _normalize_open_data(self, data: Dict, data_type: str) -> Dict[str, Any]:
        """Normalize open data to standard format"""
        if data_type == "transport":
            return {
                "name": data.get("stop_name", ""),
                "location": f"Istanbul, Turkey",
                "transport_type": data.get("stop_type"),
                "coordinates": {
                    "lat": data.get("coordinates", [None, None])[0],
                    "lng": data.get("coordinates", [None, None])[1]
                },
                "lines": data.get("lines", [])
            }
        elif data_type == "event":
            return {
                "name": data.get("title", ""),
                "location": data.get("venue", ""),
                "start_date": data.get("start_date"),
                "end_date": data.get("end_date"),
                "category": data.get("category")
            }
        
        return {}
    
    def _extract_cuisine_type(self, types: List[str]) -> str:
        """Extract cuisine type from Google Places types"""
        cuisine_mapping = {
            "turkish_cuisine": "Turkish",
            "kebab": "Turkish",
            "italian": "Italian",
            "chinese": "Chinese",
            "indian": "Indian",
            "seafood": "Seafood"
        }
        
        for place_type in types:
            if place_type in cuisine_mapping:
                return cuisine_mapping[place_type]
        
        return "International"
    
    def _validate_and_score_record(self, record: DataRecord) -> DataRecord:
        """Validate record and calculate quality score"""
        data_type = record.data_type
        normalized_data = record.normalized_data
        errors = []
        score = 1.0  # Start with perfect score
        
        if data_type in self.validation_rules:
            rules = self.validation_rules[data_type]
            
            # Check required fields
            for field in rules["required_fields"]:
                if not normalized_data.get(field):
                    errors.append(f"Missing required field: {field}")
                    score -= 0.3
            
            # Run validation functions
            for validate_func in rules["validation_functions"]:
                field_errors, field_score = validate_func(normalized_data)
                errors.extend(field_errors)
                score *= field_score
        
        # Additional quality checks
        if record.source == DataSource.GOOGLE_PLACES_API:
            score += 0.1  # Bonus for trusted source
        
        # Ensure score is between 0 and 1
        score = max(0.0, min(1.0, score))
        
        record.quality_score = score
        record.validation_errors = errors
        
        return record
    
    def _validate_name(self, data: Dict) -> Tuple[List[str], float]:
        """Validate name field"""
        errors = []
        score = 1.0
        
        name = data.get("name", "")
        if len(name) < 2:
            errors.append("Name too short")
            score *= 0.5
        elif len(name) > 100:
            errors.append("Name too long")
            score *= 0.8
        
        return errors, score
    
    def _validate_location(self, data: Dict) -> Tuple[List[str], float]:
        """Validate location field"""
        errors = []
        score = 1.0
        
        location = data.get("location", "")
        if not location:
            errors.append("Missing location")
            score *= 0.3
        elif "Istanbul" not in location and not any(district in location for district in self.istanbul_districts):
            errors.append("Location not in Istanbul")
            score *= 0.5
        
        return errors, score
    
    def _validate_rating(self, data: Dict) -> Tuple[List[str], float]:
        """Validate rating field"""
        errors = []
        score = 1.0
        
        rating = data.get("rating")
        if rating is not None:
            if not (0 <= rating <= 5):
                errors.append("Invalid rating range")
                score *= 0.7
        
        return errors, score
    
    def _validate_date(self, data: Dict) -> Tuple[List[str], float]:
        """Validate date field"""
        errors = []
        score = 1.0
        
        start_date = data.get("start_date")
        if start_date:
            try:
                datetime.fromisoformat(start_date)
            except ValueError:
                errors.append("Invalid date format")
                score *= 0.6
        
        return errors, score
    
    def _validate_opening_hours(self, data: Dict) -> Tuple[List[str], float]:
        """Validate opening hours field"""
        errors = []
        score = 1.0
        
        hours = data.get("opening_hours")
        if hours and not isinstance(hours, list):
            errors.append("Invalid opening hours format")
            score *= 0.8
        
        return errors, score
    
    def _calculate_hash(self, data: Dict) -> str:
        """Calculate hash signature for incremental updates"""
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(data_str.encode()).hexdigest()
    
    def store_records(self, records: List[DataRecord]) -> Dict[str, int]:
        """Store validated records in database"""
        stats = {"added": 0, "updated": 0, "rejected": 0}
        
        with sqlite3.connect(self.db_path) as conn:
            for record in records:
                # Check if record exists and has changed
                cursor = conn.execute(
                    "SELECT hash_signature FROM data_records WHERE id = ?",
                    (record.id,)
                )
                existing = cursor.fetchone()
                
                # Skip if unchanged
                if existing and existing[0] == record.hash_signature:
                    continue
                
                # Reject low quality records
                if record.quality_score < 0.5:
                    stats["rejected"] += 1
                    continue
                
                # Insert or update record
                conn.execute("""
                    INSERT OR REPLACE INTO data_records 
                    (id, source, data_type, raw_data, normalized_data, quality_score, 
                     validation_errors, hash_signature, status, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    record.id,
                    record.source.value,
                    record.data_type,
                    json.dumps(record.raw_data),
                    json.dumps(record.normalized_data),
                    record.quality_score,
                    json.dumps(record.validation_errors),
                    record.hash_signature,
                    "approved" if record.quality_score >= 0.9 else "pending",
                    record.last_updated.isoformat()
                ))
                
                if existing:
                    stats["updated"] += 1
                else:
                    stats["added"] += 1
            
            conn.commit()
        
        return stats
    
    def integrate_with_main_database(self) -> Dict[str, int]:
        """Integrate approved records into main database"""
        integration_stats = {"restaurants": 0, "museums": 0, "places": 0}
        
        try:
            from database import SessionLocal
            from models import Restaurant, Museum, Place
            
            db = SessionLocal()
            
            # Get approved records
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT id, data_type, normalized_data
                    FROM data_records 
                    WHERE status = 'approved'
                    AND id NOT IN (
                        SELECT id FROM data_records WHERE status = 'integrated'
                    )
                """)
                
                for record_id, data_type, normalized_data_json in cursor.fetchall():
                    normalized_data = json.loads(normalized_data_json)
                    
                    try:
                        if data_type == "restaurant":
                            restaurant = Restaurant(
                                name=normalized_data["name"],
                                description=normalized_data.get("description", ""),
                                cuisine_type=normalized_data.get("cuisine_type"),
                                district=self._extract_district(normalized_data["location"]),
                                rating=normalized_data.get("rating"),
                                phone=normalized_data.get("phone")
                            )
                            db.add(restaurant)
                            integration_stats["restaurants"] += 1
                            
                        elif data_type == "museum":
                            museum = Museum(
                                name=normalized_data["name"],
                                description=normalized_data.get("description", ""),
                                category=normalized_data.get("category"),
                                district=self._extract_district(normalized_data["location"]),
                                rating=normalized_data.get("rating")
                            )
                            db.add(museum)
                            integration_stats["museums"] += 1
                        
                        # Mark as integrated
                        conn.execute(
                            "UPDATE data_records SET status = 'integrated' WHERE id = ?",
                            (record_id,)
                        )
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è Integration error for {record_id}: {e}")
                
                conn.commit()
            
            db.commit()
            db.close()
            
            print(f"‚úÖ Integrated records: {integration_stats}")
            
        except Exception as e:
            print(f"‚ùå Database integration error: {e}")
        
        return integration_stats
    
    def update_vector_embeddings(self):
        """Update vector embeddings for new/changed records"""
        if not VECTOR_SYSTEM_AVAILABLE:
            print("‚ö†Ô∏è Vector system not available")
            return
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT id, data_type, normalized_data
                    FROM data_records 
                    WHERE status = 'approved'
                    AND updated_at > datetime('now', '-1 day')
                """)
                
                updated_count = 0
                for record_id, data_type, normalized_data_json in cursor.fetchall():
                    normalized_data = json.loads(normalized_data_json)
                    
                    # Create content for embedding
                    content = f"{normalized_data['name']} {normalized_data.get('description', '')} {normalized_data.get('category', '')}"
                    
                    # Add to vector system
                    if vector_embedding_system.add_document(
                        record_id, content, normalized_data, data_type
                    ):
                        updated_count += 1
                
                print(f"‚úÖ Updated {updated_count} vector embeddings")
                
        except Exception as e:
            print(f"‚ùå Vector embedding update error: {e}")
    
    def _extract_district(self, location: str) -> str:
        """Extract Istanbul district from location string"""
        for district in self.istanbul_districts:
            if district.lower() in location.lower():
                return district
        
        # Default district if none found
        return "Istanbul"
    
    async def run_full_pipeline(self) -> Dict[str, Any]:
        """Run complete data pipeline"""
        pipeline_id = f"pipeline_{int(time.time())}"
        start_time = time.time()
        
        print(f"üöÄ Starting full data pipeline: {pipeline_id}")
        
        all_records = []
        
        # Collect from all sources
        try:
            # Google Places data
            restaurant_records = await self.collect_google_places_data("restaurants Istanbul", "restaurant")
            museum_records = await self.collect_google_places_data("museums Istanbul", "museum")
            all_records.extend(restaurant_records + museum_records)
            
            # Open data
            transport_records = await self.collect_istanbul_open_data("transport")
            event_records = await self.collect_istanbul_open_data("event")
            all_records.extend(transport_records + event_records)
            
        except Exception as e:
            print(f"‚ùå Data collection error: {e}")
        
        # Store records
        storage_stats = self.store_records(all_records)
        
        # Integrate with main database
        integration_stats = self.integrate_with_main_database()
        
        # Update vector embeddings
        self.update_vector_embeddings()
        
        # Log pipeline run
        duration = time.time() - start_time
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO processing_logs 
                (pipeline_run_id, source, records_processed, records_added, 
                 records_updated, records_rejected, duration_seconds)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                pipeline_id,
                "all_sources",
                len(all_records),
                storage_stats["added"],
                storage_stats["updated"],
                storage_stats["rejected"],
                duration
            ))
            conn.commit()
        
        result = {
            "pipeline_id": pipeline_id,
            "duration_seconds": duration,
            "records_collected": len(all_records),
            "storage_stats": storage_stats,
            "integration_stats": integration_stats,
            "success": True
        }
        
        print(f"‚úÖ Pipeline completed: {result}")
        return result
    
    async def close(self):
        """Clean up resources"""
        if self.session:
            await self.session.close()

# Global pipeline instance
enhanced_data_pipeline = EnhancedDataPipeline()

def schedule_pipeline_runs():
    """Schedule regular pipeline runs"""
    # Schedule daily runs at 3 AM
    schedule.every().day.at("03:00").do(
        lambda: asyncio.run(enhanced_data_pipeline.run_full_pipeline())
    )
    
    # Schedule incremental updates every 6 hours
    schedule.every(6).hours.do(
        lambda: asyncio.run(enhanced_data_pipeline.run_full_pipeline())
    )
    
    print("‚è∞ Pipeline scheduling configured")

if __name__ == "__main__":
    # Test the enhanced pipeline
    print("üß™ Testing Enhanced Data Pipeline...")
    
    async def test_pipeline():
        pipeline = EnhancedDataPipeline()
        result = await pipeline.run_full_pipeline()
        print(f"‚úÖ Test completed: {result}")
        await pipeline.close()
    
    asyncio.run(test_pipeline())
    print("‚úÖ Enhanced Data Pipeline is working correctly!")
