"""
prompts.py - Prompt Engineering System

Advanced prompt construction for optimal LLM performance.

Features:
- Intent-specific prompts
- Dynamic context injection
- Conversation history formatting
- Multi-language support
- Token optimization
- Few-shot examples
- Advanced prompt engineering for low-signal scenarios (Phase 4 Priority 3)

Author: AI Istanbul Team
Date: November 2025
"""

import logging
from typing import Dict, Any, Optional, List

# Import Phase 4 Priority 3: Advanced Prompt Engineering
try:
    from .advanced_prompts import get_prompt_engineer
    ADVANCED_PROMPTS_AVAILABLE = True
except ImportError:
    ADVANCED_PROMPTS_AVAILABLE = False
    logger.warning("âš ï¸ Advanced prompt engineering module not available")

logger = logging.getLogger(__name__)


class PromptBuilder:
    """
    Advanced prompt engineering system.
    
    Builds optimized prompts based on:
    - Detected signals/intents
    - Available context (database, RAG, services)
    - Conversation history
    - Language preferences
    """
    
    def __init__(
        self,
        system_prompts: Optional[Dict[str, str]] = None,
        intent_prompts: Optional[Dict[str, str]] = None
    ):
        """
        Initialize prompt builder.
        
        Args:
            system_prompts: Custom system prompts
            intent_prompts: Custom intent-specific prompts
        """
        self.system_prompts = system_prompts or self._default_system_prompts()
        self.intent_prompts = intent_prompts or self._default_intent_prompts()
        
        logger.info("âœ… Prompt Builder initialized")
    
    def _default_system_prompts(self) -> Dict[str, str]:
        """Simplified system prompts optimized for Llama 3.1 8B."""
        
        # ENGLISH PROMPT
        english_prompt = """You are KAM, an expert Istanbul tour guide.

CRITICAL: Answer in ENGLISH only.

GUIDELINES:
- Use the information provided in the CONTEXT below
- Be specific with names, metro lines (M1, M2, T1, F1), and locations
- For directions: Give step-by-step transit instructions
- Keep answers focused and practical

ISTANBUL TRANSPORTATION:
Metro: M1, M2, M3, M4, M5, M6, M7, M9, M11
Tram: T1, T4, T5
Funicular: F1 (Taksim-KabataÅŸ), F2 (KarakÃ¶y-TÃ¼nel)
Marmaray: Underground rail crossing Bosphorus
Ferries: KadÄ±kÃ¶y-KarakÃ¶y, KadÄ±kÃ¶y-EminÃ¶nÃ¼, ÃœskÃ¼dar-EminÃ¶nÃ¼

Start your answer immediately without repeating these instructions."""
        
        # TURKISH PROMPT
        turkish_prompt = """Sen KAM, uzman bir Ä°stanbul tur rehberisin.

Ã–NEMLÄ°: Sadece TÃœRKÃ‡E cevap ver.

KURALLAR:
- AÅŸaÄŸÄ±daki BAÄLAM bilgilerini kullan
- Metro hatlarÄ± (M1, M2, T1, F1) ve yer isimleri belirt
- Yol tarifi iÃ§in: AdÄ±m adÄ±m ulaÅŸÄ±m talimatlarÄ± ver
- CevaplarÄ± odaklÄ± ve pratik tut

Ä°STANBUL ULAÅIM:
Metro: M1, M2, M3, M4, M5, M6, M7, M9, M11
Tramvay: T1, T4, T5
FÃ¼nikÃ¼ler: F1 (Taksim-KabataÅŸ), F2 (KarakÃ¶y-TÃ¼nel)
Marmaray: BoÄŸaz'Ä± geÃ§en yeraltÄ± treni
Vapur: KadÄ±kÃ¶y-KarakÃ¶y, KadÄ±kÃ¶y-EminÃ¶nÃ¼, ÃœskÃ¼dar-EminÃ¶nÃ¼

Bu talimatlarÄ± tekrarlama, cevabÄ±nÄ± hemen baÅŸlat."""
        
        return {
            'en': english_prompt,
            'tr': turkish_prompt,
            'fr': english_prompt.replace('ENGLISH only', 'FRENCH only'),
            'ru': english_prompt.replace('ENGLISH only', 'RUSSIAN only'),
            'de': english_prompt.replace('ENGLISH only', 'GERMAN only'),
            'ar': english_prompt.replace('ENGLISH only', 'ARABIC only')
        }
    
    def _default_intent_prompts(self) -> Dict[str, str]:
        """Intent-specific prompts - NOT USED with Llama 3.1 8B (LLM handles intent detection)."""
        # Keeping this empty - Llama 3.1 8B is smart enough to understand user intent
        # without explicit signal-based instructions
        return {}
    
    def build_prompt(
        self,
        query: str,
        signals: Dict[str, bool],
        context: Dict[str, Any],
        conversation_context: Optional[Dict[str, Any]] = None,
        language: str = "en",
        user_location: Optional[Dict[str, float]] = None,
        enable_intent_classification: bool = False,
        signal_confidence: float = 1.0
    ) -> str:
        """
        Build complete optimized prompt.
        
        Let Llama 3.1 8B handle intent detection naturally from the query.
        We just provide context and let the LLM figure out what the user needs.
        
        Args:
            query: User query
            signals: Detected signals (kept for backwards compatibility, but not heavily used)
            context: Built context (database, RAG, services)
            conversation_context: Conversation history
            language: Response language
            user_location: User's GPS coordinates (if available)
            enable_intent_classification: Enable LLM intent classification (Priority 2)
            signal_confidence: Overall signal detection confidence (Priority 3)
            
        Returns:
            Complete prompt string
        """
        prompt_parts = []
        
        # 1. System prompt (contains all the intelligence)
        system_prompt = self.system_prompts.get(language, self.system_prompts['en'])
        
        # ADD GPS CONTEXT if available - for ANY location-based query
        if user_location and isinstance(user_location, dict) and 'lat' in user_location and 'lon' in user_location:
            # Check if this is any location-based query (routing, nearby, recommendations, etc.)
            is_location_query = any([
                signals.get('needs_gps_routing'),
                signals.get('needs_directions'),
                signals.get('needs_transportation'),
                signals.get('needs_restaurant'),
                signals.get('needs_attraction'),
                signals.get('needs_hidden_gems'),
                signals.get('needs_shopping'),
                signals.get('needs_nightlife'),
                signals.get('needs_events'),
                signals.get('needs_daily_life'),
                'nearby' in query.lower(),
                'near me' in query.lower(),
                'close to me' in query.lower(),
                'around me' in query.lower(),
                'around here' in query.lower(),
                'how' in query.lower() and ('get' in query.lower() or 'go' in query.lower())
            ])
            
            if is_location_query:
                try:
                    lat = float(user_location['lat'])
                    lon = float(user_location['lon'])
                    
                    # Try to identify if user is on Asian or European side
                    side = "Asian" if lon > 29.05 else "European"
                    
                    # ROUTING QUERIES GET EXTRA DETAILED GPS CONTEXT
                    is_routing = signals.get('needs_gps_routing') or signals.get('needs_directions') or ('how' in query.lower() and any(w in query.lower() for w in ['get', 'go', 'reach']))
                    
                    if is_routing:
                        system_prompt += f"\n\nGPS ROUTING REQUEST:"
                        system_prompt += f"\nUser starting location: {lat:.5f}, {lon:.5f} ({side} side of Istanbul)"
                        system_prompt += f"\nGive specific step-by-step transit directions from this GPS point."
                    else:
                        system_prompt += f"\n\nUser GPS location: {lat}, {lon} ({side} side)"
                        system_prompt += f"\nUse for nearby recommendations."
                except (ValueError, TypeError):
                    # Invalid coordinates - skip GPS status
                    pass
        
        prompt_parts.append(system_prompt)
        
        # 2. Conversation context (if available)
        if conversation_context:
            conv_formatted = self._format_conversation_context(conversation_context)
            if conv_formatted:
                prompt_parts.append("\n## Previous Conversation:")
                prompt_parts.append(conv_formatted)
        
        # 3. Database context
        if context.get('database'):
            prompt_parts.append("\n## Database Information:")
            prompt_parts.append(context['database'])
        
        # 4. RAG context
        if context.get('rag'):
            prompt_parts.append("\n## Additional Context:")
            prompt_parts.append(context['rag'])
        
        # 5. Service context (weather, events, hidden gems)
        service_context = self._format_service_context(context.get('services', {}))
        if service_context:
            prompt_parts.append("\n## Real-Time Information:")
            prompt_parts.append(service_context)
        
        # 6. Map reference (if available)
        if context.get('map_data'):
            map_data = context['map_data']
            has_origin = map_data.get('has_origin', False)
            has_destination = map_data.get('has_destination', False)
            origin_name = map_data.get('origin_name')
            destination_name = map_data.get('destination_name')
            
            prompt_parts.append("\n## Map:")
            prompt_parts.append("A map will be shown to the user.")
            
            if has_origin and has_destination:
                prompt_parts.append(f"\nRoute: {origin_name} to {destination_name}")
                prompt_parts.append(f"Provide step-by-step transit directions with specific metro/tram lines.")
            elif has_destination and not has_origin:
                prompt_parts.append(f"Destination: {destination_name}")
            
            prompt_parts.append("Mention the map in your response.")
        
        # DISABLED: Intent classification, low-confidence, and multi-intent prompts cause template artifacts
        # These features are currently disabled to keep responses clean and focused
        
        # 7. Language reminder + User query
        # Add explicit language reminder right before the answer section
        lang_name_map = {
            'en': 'English',
            'tr': 'Turkish',
            'fr': 'French',
            'ru': 'Russian',
            'de': 'German',
            'ar': 'Arabic'
        }
        lang_name = lang_name_map.get(language, 'English')
        
        prompt_parts.append(f"\n---\n\nğŸŒ REMEMBER: Answer in {lang_name} only.")
        prompt_parts.append(f"\nUser Question: {query}\n\nYour Answer:")

        
        # Join all parts
        full_prompt = "\n".join(prompt_parts)
        
        logger.debug(f"Built prompt: {len(full_prompt)} chars")
        
        return full_prompt
    
    def _build_intent_instructions(self, active_signals: List[str]) -> str:
        """Build intent-specific instructions."""
        instructions = []
        
        for signal in active_signals:
            if signal in self.intent_prompts:
                instructions.append(self.intent_prompts[signal])
        
        return "\n".join(instructions) if instructions else ""
    
    def _format_conversation_context(
        self,
        conversation_context: Dict[str, Any]
    ) -> str:
        """Format conversation history for prompt."""
        if not conversation_context or not conversation_context.get('history'):
            return ""
        
        formatted = []
        history = conversation_context['history']
        
        for turn in history[-3:]:  # Last 3 turns
            role = turn.get('role', 'user')
            content = turn.get('content', '')
            
            if role == 'user':
                formatted.append(f"User: {content}")
            elif role == 'assistant':
                formatted.append(f"Assistant: {content}")
        
        return "\n".join(formatted) if formatted else ""
    
    def _format_service_context(self, services: Dict[str, Any]) -> str:
        """Format service context (weather, events, etc.)."""
        if not services:
            return ""
        
        formatted = []
        
        # Weather
        if 'weather' in services:
            formatted.append(f"Weather: {services['weather']}")
        
        # Events
        if 'events' in services:
            formatted.append(f"Events:\n{services['events']}")
        
        # Hidden Gems
        if 'hidden_gems' in services:
            formatted.append(f"Hidden Gems:\n{services['hidden_gems']}")
        
        return "\n\n".join(formatted) if formatted else ""
    
    def _get_response_instructions(
        self,
        language: str,
        signals: Dict[str, bool]
    ) -> str:
        """Get response format instructions."""
        # Language-specific response instructions
        language_instructions = {
            'en': "Please respond in English.",
            'tr': "LÃ¼tfen TÃ¼rkÃ§e olarak yanÄ±t verin.",
            'fr': "Veuillez rÃ©pondre en franÃ§ais.",
            'ru': "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ğ¹Ñ‚Ğµ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.",
            'de': "Bitte antworten Sie auf Deutsch.",
            'ar': "ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."
        }
        
        base = language_instructions.get(language, language_instructions['en'])
        
        # Add signal-specific instructions
        if signals.get('needs_map') or signals.get('needs_gps_routing'):
            base += " Reference the provided map to help guide the user."
        
        if signals.get('needs_transportation'):
            base += " Provide step-by-step directions."
        
        if signals.get('needs_restaurant'):
            base += " Recommend 2-3 specific restaurants with details."
        
        return base
    
    def build_few_shot_prompt(
        self,
        query: str,
        examples: List[Dict[str, str]],
        context: Optional[str] = None,
        language: str = "en"
    ) -> str:
        """
        Build prompt with few-shot examples.
        
        Args:
            query: User query
            examples: List of {'query': ..., 'response': ...}
            context: Optional context
            language: Language code
            
        Returns:
            Few-shot prompt
        """
        prompt_parts = []
        
        # System prompt
        system_prompt = self.system_prompts.get(language, self.system_prompts['en'])
        prompt_parts.append(system_prompt)
        
        # Few-shot examples
        if examples:
            prompt_parts.append("\n## Examples:")
            for i, example in enumerate(examples, 1):
                prompt_parts.append(f"\nExample {i}:")
                prompt_parts.append(f"User: {example['query']}")
                prompt_parts.append(f"Assistant: {example['response']}")
        
        # Context
        if context:
            prompt_parts.append(f"\n## Context:\n{context}")
        
        # User query
        prompt_parts.append(f"\n## User Question:\n{query}")
        prompt_parts.append("\n## Response:")
        
        return "\n".join(prompt_parts)
    
    def build_chain_of_thought_prompt(
        self,
        query: str,
        context: Optional[str] = None,
        language: str = "en"
    ) -> str:
        """
        Build prompt for chain-of-thought reasoning.
        
        Args:
            query: User query
            context: Optional context
            language: Language code
            
        Returns:
            Chain-of-thought prompt
        """
        thinking_instructions = {
            'en': "Let's think step by step, then provide your answer.",
            'tr': "Ã–nce adÄ±m adÄ±m dÃ¼ÅŸÃ¼nÃ¼n, sonra yanÄ±t verin.",
            'fr': "RÃ©flÃ©chissons Ã©tape par Ã©tape, puis fournissez votre rÃ©ponse.",
            'ru': "Ğ”Ğ°Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ´ÑƒĞ¼Ğ°ĞµĞ¼ ÑˆĞ°Ğ³ Ğ·Ğ° ÑˆĞ°Ğ³Ğ¾Ğ¼, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ°Ğ´Ğ¸Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚.",
            'de': "Lassen Sie uns Schritt fÃ¼r Schritt denken und dann Ihre Antwort geben.",
            'ar': "Ø¯Ø¹Ù†Ø§ Ù†ÙÙƒØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©ØŒ Ø«Ù… Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨ØªÙƒ."
        }
        
        thinking_instruction = thinking_instructions.get(language, thinking_instructions['en'])
        
        prompt_parts = [
            self.system_prompts.get(language, self.system_prompts['en']),
            f"\n## Approach:\n{thinking_instruction}"
        ]
        
        if context:
            prompt_parts.append(f"\n## Context:\n{context}")
        
        prompt_parts.append(f"\n## Question:\n{query}")
        prompt_parts.append("\n## Reasoning:")
        
        return "\n".join(prompt_parts)
    
    def optimize_prompt_length(
        self,
        prompt: str,
        max_tokens: int = 2000
    ) -> str:
        """
        Optimize prompt length to fit within token limits.
        
        Args:
            prompt: Original prompt
            max_tokens: Maximum allowed tokens
            
        Returns:
            Optimized prompt
        """
        # Simple character-based approximation (1 token â‰ˆ 4 chars)
        max_chars = max_tokens * 4
        
        if len(prompt) <= max_chars:
            return prompt
        
        # Truncate context sections intelligently
        # TODO: Implement smarter truncation (preserve system prompt, truncate context)
        logger.warning(f"Prompt too long ({len(prompt)} chars), truncating to {max_chars}")
        
        return prompt[:max_chars] + "\n\n[Context truncated for length]"
    
    def add_safety_guidelines(self, prompt: str, language: str = "en") -> str:
        """
        Add safety and ethical guidelines to prompt.
        
        Args:
            prompt: Base prompt
            language: Language code
            
        Returns:
            Prompt with safety guidelines
        """
        safety_guidelines = {
            'en': """
## Safety Guidelines:
- Do not provide harmful, illegal, or inappropriate content
- Respect cultural sensitivities
- Do not request or share personal information
- Do not provide medical, legal, or financial advice""",

            'tr': """
## GÃ¼venlik KurallarÄ±:
- ZararlÄ±, yasadÄ±ÅŸÄ± veya uygunsuz iÃ§erik saÄŸlamayÄ±n
- KÃ¼ltÃ¼rel hassasiyetlere saygÄ± gÃ¶sterin
- KiÅŸisel bilgi istemeyin veya paylaÅŸmayÄ±n
- TÄ±bbi, hukuki veya finansal tavsiye vermeyin""",

            'fr': """
## Directives de sÃ©curitÃ©:
- Ne fournissez pas de contenu nuisible, illÃ©gal ou inappropriÃ©
- Respectez les sensibilitÃ©s culturelles
- Ne demandez pas et ne partagez pas d'informations personnelles
- Ne fournissez pas de conseils mÃ©dicaux, juridiques ou financiers""",

            'ru': """
## ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸:
- ĞĞµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞ¹Ñ‚Ğµ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğ¹, Ğ½ĞµĞ·Ğ°ĞºĞ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑƒĞ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚
- Ğ£Ğ²Ğ°Ğ¶Ğ°Ğ¹Ñ‚Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸
- ĞĞµ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ¸ Ğ½Ğµ Ğ´ĞµĞ»Ğ¸Ñ‚ĞµÑÑŒ Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹
- ĞĞµ Ğ´Ğ°Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ…, ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ»Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¾Ğ²ĞµÑ‚Ğ¾Ğ²""",

            'de': """
## Sicherheitsrichtlinien:
- Bieten Sie keine schÃ¤dlichen, illegalen oder unangemessenen Inhalte an
- Respektieren Sie kulturelle Empfindlichkeiten
- Fordern Sie keine persÃ¶nlichen Informationen an und geben Sie keine weiter
- Geben Sie keine medizinischen, rechtlichen oder finanziellen RatschlÃ¤ge""",

            'ar': """
## Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„Ø³Ù„Ø§Ù…Ø©:
- Ù„Ø§ ØªÙ‚Ø¯Ù… Ù…Ø­ØªÙˆÙ‰ Ø¶Ø§Ø± Ø£Ùˆ ØºÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø£Ùˆ ØºÙŠØ± Ù„Ø§Ø¦Ù‚
- Ø§Ø­ØªØ±Ù… Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ©
- Ù„Ø§ ØªØ·Ù„Ø¨ Ø£Ùˆ ØªØ´Ø§Ø±Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø´Ø®ØµÙŠØ©
- Ù„Ø§ ØªÙ‚Ø¯Ù… Ù†ØµØ§Ø¦Ø­ Ø·Ø¨ÙŠØ© Ø£Ùˆ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø£Ùˆ Ù…Ø§Ù„ÙŠØ©"""
        }
        
        safety = safety_guidelines.get(language, safety_guidelines['en'])
        
        return f"{prompt}\n{safety}"
