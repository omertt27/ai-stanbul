# Week 7-8 LightGBM Ranker Quick Start Guide

## ğŸ¯ Overview

This guide covers the **LightGBM Ranker** implementation for Week 7-8 of the budget-optimized recommendation system. The LightGBM ranker complements the NCF model by providing fast, CPU-based ranking with rich feature engineering.

## ğŸ“Š Architecture

```
User Query â†’ Feature Engineering â†’ LightGBM Ranker â†’ Ranked Results
                     â†“
            [User Features, Item Features, 
             Temporal Features, NCF Embeddings]
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install lightgbm scikit-learn pandas numpy joblib
```

### 2. Test Feature Engineering

```bash
cd backend/ml/data
python prepare_ranker_features.py
```

**Expected output:**
- âœ… Synthetic data created
- âœ… Features extracted (10-20 features)
- âœ… Train/val splits created
- âœ… Feature statistics saved

### 3. Train LightGBM Ranker

```bash
cd backend/ml/training
python train_lightgbm_ranker.py --use_synthetic --n_estimators 100
```

**Expected output:**
- ğŸš€ Training completes in 1-3 minutes
- ğŸ“Š Model size: <100MB
- ğŸ“ˆ NDCG@10: 0.70-0.85
- ğŸ’¾ Model saved to `models/ranker/lightgbm_ranker.pkl`

### 4. Test Inference

```bash
cd backend/ml/serving
python lightgbm_ranker_inference.py
```

**Expected output:**
- âœ… Model loads successfully
- âœ… Single user ranking works
- âœ… Batch ranking works
- âš¡ Inference: <10ms per query

### 5. Run Full Test Suite

```bash
cd backend/ml/tests
python test_lightgbm_ranker.py
```

**Expected output:**
- âœ… 12+ tests pass
- â±ï¸ Training: <60s
- âš¡ Inference: <1s for 100 items

## ğŸ“ File Structure

```
backend/ml/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ lightgbm_ranker.py          # LightGBM ranker model
â”œâ”€â”€ data/
â”‚   â””â”€â”€ prepare_ranker_features.py  # Feature engineering
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train_lightgbm_ranker.py    # Training script
â”œâ”€â”€ serving/
â”‚   â””â”€â”€ lightgbm_ranker_inference.py # Inference service
â””â”€â”€ tests/
    â””â”€â”€ test_lightgbm_ranker.py     # Test suite
```

## ğŸ¨ Features

### Feature Engineering (`prepare_ranker_features.py`)

**User Features:**
- Number of interactions
- Average rating
- Rating std deviation
- Days since last interaction

**Item Features:**
- Popularity (number of interactions)
- Log-scaled popularity
- Average rating
- Rating std deviation

**Temporal Features:**
- Hour of day
- Day of week
- Is weekend
- Month

**User-Item Interaction Features:**
- Rating deviation from user mean
- Rating deviation from item mean

**NCF Embeddings (Optional):**
- Dot product of user/item embeddings
- Cosine similarity
- Hadamard product (element-wise)

### Model Parameters

**Default configuration (optimized for T4):**
```python
num_leaves = 31          # Moderate tree complexity
learning_rate = 0.05     # Conservative learning
n_estimators = 100       # 100 boosting rounds
num_threads = 8          # Multi-threading
feature_fraction = 0.9   # Feature sampling
bagging_fraction = 0.8   # Data sampling
min_child_samples = 20   # Regularization
```

**Training objectives:**
- Objective: LambdaRank (learning to rank)
- Metric: NDCG@5, NDCG@10, NDCG@20
- Early stopping: 10 rounds

## ğŸ“Š Training with Real Data

### 1. Prepare Your Data

Create CSV files with the following structure:

**interactions.csv:**
```csv
user_id,item_id,rating,timestamp
1,100,4.5,1609459200
1,101,5.0,1609545600
2,100,3.5,1609632000
...
```

**users.csv (optional):**
```csv
user_id,age,gender
1,25,M
2,30,F
...
```

**items.csv (optional):**
```csv
item_id,category,price
100,restaurant,25.50
101,attraction,15.00
...
```

### 2. Train with Your Data

```bash
python train_lightgbm_ranker.py \
  --data_dir /path/to/your/data \
  --output_dir models/ranker \
  --n_estimators 100 \
  --learning_rate 0.05
```

### 3. With NCF Embeddings (Optional)

If you have trained the NCF model (Week 5-6):

```bash
# First, extract NCF embeddings
cd backend/ml/serving
python -c "
from lightweight_ncf_inference import NCFInferenceService
service = NCFInferenceService('models/ncf/best_model.pt')
embeddings = service.export_embeddings()
import pickle
with open('models/ncf/embeddings.pkl', 'wb') as f:
    pickle.dump(embeddings, f)
"

# Then train ranker with embeddings
python train_lightgbm_ranker.py \
  --data_dir data/ranker \
  --output_dir models/ranker \
  --ncf_embeddings models/ncf/embeddings.pkl
```

## ğŸ¯ Inference Usage

### Python API

```python
from backend.ml.serving.lightgbm_ranker_inference import RankerInferenceService

# Initialize service
service = RankerInferenceService(
    model_path='models/ranker/lightgbm_ranker.pkl',
    feature_stats_path='models/ranker/feature_stats.pkl',
    ncf_embeddings_path='models/ncf/embeddings.pkl',  # Optional
    cache_size=1000
)

# Rank items for a user
user_id = 42
candidate_items = [1, 5, 10, 20, 50, 100]

ranked_items = service.rank_items_for_user(
    user_id=user_id,
    item_ids=candidate_items,
    user_stats={  # Optional, will use cache if not provided
        'n_interactions': 10,
        'avg_rating': 4.2,
        'std_rating': 0.5,
        'days_since_last_interaction': 2.0
    },
    item_stats={  # Optional
        1: {'popularity': 100, 'avg_rating': 4.5, ...},
        5: {'popularity': 50, 'avg_rating': 4.0, ...},
        ...
    }
)

# Results: [(item_id, score), ...] sorted by score descending
print(ranked_items)
# [(50, 4.85), (10, 4.72), (100, 4.51), ...]
```

### Batch Inference

```python
# Rank for multiple users
user_ids = [1, 2, 3]
item_ids_per_user = [
    [10, 20, 30],
    [40, 50],
    [60, 70, 80, 90]
]

batch_results = service.rank_items_batch(
    user_ids=user_ids,
    item_ids_per_user=item_ids_per_user
)

for user_id, ranked_items in zip(user_ids, batch_results):
    print(f"User {user_id}: {ranked_items[:3]}")
```

### Update Cached Statistics

```python
# Update user stats
service.update_user_stats(user_id=42, stats={
    'n_interactions': 15,
    'avg_rating': 4.3,
    'std_rating': 0.6,
    'days_since_last_interaction': 0.5
})

# Update item stats
service.update_item_stats(item_id=100, stats={
    'popularity': 120,
    'log_popularity': 4.79,
    'avg_rating': 4.6,
    'std_rating': 0.4
})

# Batch update
service.batch_update_stats(
    user_stats={1: {...}, 2: {...}},
    item_stats={100: {...}, 101: {...}}
)
```

## ğŸ”§ Troubleshooting

### Issue: `ModuleNotFoundError: No module named 'lightgbm'`

**Solution:**
```bash
pip install lightgbm
```

### Issue: Training is slow

**Solutions:**
1. Reduce `n_estimators`: `--n_estimators 50`
2. Increase `num_threads`: Modify in `lightgbm_ranker.py`
3. Use smaller dataset for testing

### Issue: Low NDCG scores

**Solutions:**
1. Check data quality (need diverse ratings)
2. Increase `n_estimators` for more training
3. Tune hyperparameters:
   - `num_leaves`: 31-63
   - `learning_rate`: 0.01-0.1
   - `min_child_samples`: 10-30

### Issue: Memory errors

**Solutions:**
1. Reduce feature dimensions (remove NCF embeddings)
2. Use smaller batch sizes
3. Sample training data

## ğŸ“ˆ Performance Benchmarks

**On M2 Pro (Development):**
- Training time: 1-3 minutes (10K interactions, 100 estimators)
- Model size: 50-100MB
- Inference: 5-10ms per query (10 candidates)
- Throughput: 100-200 queries/sec (single thread)

**On T4 GPU (Production):**
- Training time: 2-5 minutes (same dataset)
- Model size: Same (CPU-based)
- Inference: 3-8ms per query
- Throughput: 120-300 queries/sec (8 threads)

## ğŸ¯ Integration with NCF (Week 10)

The LightGBM ranker is designed to work alongside the NCF model:

1. **Two-stage ranking:**
   - NCF: Fast candidate generation (top 100-200 items)
   - LightGBM: Re-ranking with rich features (top 20 items)

2. **Ensemble:**
   - Combine NCF and LightGBM scores
   - Weighted average: `0.7 * ncf_score + 0.3 * lgb_score`

3. **A/B Testing:**
   - NCF alone vs LightGBM alone vs Ensemble
   - Monitor NDCG, MRR, and business metrics

## ğŸš€ Next Steps

1. âœ… **Week 7-8 Complete**: LightGBM Ranker implemented
2. ğŸ”„ **Week 9**: Optimize and tune hyperparameters
3. ğŸ¯ **Week 10**: Ensemble NCF + LightGBM
4. ğŸ“Š **Week 11-12**: A/B testing and deployment

## ğŸ“š Resources

- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [Learning to Rank Guide](https://lightgbm.readthedocs.io/en/latest/Parameters.html#learning-task-parameters)
- [Feature Engineering Best Practices](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html)

## ğŸ’¡ Tips

1. **Feature engineering is key**: Spend time creating good features
2. **Use NCF embeddings**: They significantly improve performance
3. **Monitor feature importance**: Remove low-importance features
4. **Cross-validate**: Use proper train/val/test splits
5. **Cache statistics**: Precompute user/item stats for fast inference
6. **Tune hyperparameters**: Grid search on validation set
7. **Profile performance**: Measure training and inference times

## âœ… Validation Checklist

- [ ] Feature engineering creates expected number of features
- [ ] Training completes in <5 minutes
- [ ] Model size <100MB
- [ ] Inference <10ms per query
- [ ] NDCG@10 >0.7 on validation set
- [ ] All tests pass
- [ ] Integration with NCF works (if available)

---

**Status:** âœ… Week 7-8 LightGBM Ranker Complete  
**Next:** Week 9-10 Ensemble Integration

## ğŸ”— LLM System Integration

### Integrated Recommendation Service

The LightGBM ranker is fully integrated with the AI Istanbul LLM system through the **Integrated Recommendation Service**:

```python
from backend.ml.serving.integrated_recommendation_service import IntegratedRecommendationService

# Initialize with all components
service = IntegratedRecommendationService(
    lightgbm_model_path='models/ranker/lightgbm_ranker.pkl',
    lightgbm_feature_stats_path='models/ranker/feature_stats.pkl',
    ncf_model_path='models/ncf/best_model.pt',
    ncf_embeddings_path='models/ncf/embeddings.pkl',
    use_llm=True,
    ensemble_weights={'llm': 0.4, 'ncf': 0.3, 'lightgbm': 0.3}
)

# Get recommendations
recommendations = await service.get_recommendations(
    user_id=42,
    query="Find me authentic Turkish breakfast places",
    context={
        'location': {'lat': 41.0082, 'lon': 28.9784},
        'preferences': ['authentic', 'local']
    },
    top_k=10
)
```

### Production API Usage

```bash
# Get integrated recommendations
curl -X POST "http://localhost:8000/api/ml/recommendations" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": 42,
    "query": "authentic Turkish breakfast near Sultanahmet",
    "location": {"lat": 41.0082, "lon": 28.9784},
    "preferences": ["authentic", "local", "breakfast"],
    "top_k": 10
  }'
```

**Response includes:**
- **Final scores** - Ensemble of LLM, NCF, and LightGBM
- **Component scores** - Individual scores from each model
- **Multi-source explanations** - Why each item was recommended
- **Confidence levels** - How confident the system is

### Frontend Integration Example

```jsx
import React, { useState } from 'react';
import axios from 'axios';

function AIRecommendations() {
  const [recommendations, setRecommendations] = useState([]);
  
  const getRecommendations = async (query) => {
    const response = await axios.post('/api/ml/recommendations', {
      user_id: getUserId(),
      query: query,
      location: getCurrentLocation(),
      top_k: 10
    });
    
    setRecommendations(response.data.recommendations);
  };
  
  return (
    <div>
      {recommendations.map(rec => (
        <div key={rec.item_id} className="recommendation-card">
          <h3>{rec.name}</h3>
          <div className="scores">
            <span>LLM: {(rec.component_scores.llm * 100).toFixed(0)}%</span>
            <span>CF: {(rec.component_scores.ncf * 100).toFixed(0)}%</span>
            <span>Rank: {(rec.component_scores.lightgbm * 100).toFixed(0)}%</span>
          </div>
          <p>{rec.explanation}</p>
        </div>
      ))}
    </div>
  );
}
```

### Architecture Flow

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Candidate Generation           â”‚
â”‚  â€¢ Parse query with GPT-4           â”‚
â”‚  â€¢ Context-aware retrieval          â”‚
â”‚  â€¢ Generate 100-200 candidates      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NCF Collaborative Filtering        â”‚
â”‚  â€¢ Score candidates                 â”‚
â”‚  â€¢ User-item similarity             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LightGBM Re-ranking                â”‚
â”‚  â€¢ Rich feature extraction          â”‚
â”‚  â€¢ Gradient boosting ranking        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ensemble & Explanation             â”‚
â”‚  â€¢ Weighted combination             â”‚
â”‚  â€¢ Multi-source explanations        â”‚
â”‚  â€¢ Final Top K                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**See [LLM_ML_INTEGRATION_GUIDE.md](/LLM_ML_INTEGRATION_GUIDE.md) for complete integration details.**
